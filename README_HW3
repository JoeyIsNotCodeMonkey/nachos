In this assignment, we implemented different CPU scheduling.

Even though, in the original version of Nachos, only preemptive scheduling will start the timer,
but for the purpose of estimating current system tick, we have permanently set the timer to true 
so we can get the current system time in case if any scheduling requires information about current system time.

When we write our test driver class, we create a while loop and initially set the count to 0, this represents 
the number of processes that we will generate during the testing. Usually, we set the 100<count<200 and collect data for the graph.
As for the S value using in each sleep, we got the load average close to 6 using S = 500 and stackSize = 128 to avoid unstable results.

Every time the test driver creates a new process, it executes our C test program(hw3_schedulertest1) which will take the number_of_iteration 
that is randomly generated by the formula given in the manual and executes for about 10 times the iteration.

Here's short explanations for each of the scheduling process:

Round Robin - basically use the same queue structure as FIFO but in the scheduler class, the time interrupt class will check
			  for each process's quantum counter. When the counter reaches 0, the process will be yield and the counter will be
			  set back to 1000.

SPN 		- uses linked list for queueing and every time someone calls poll(), we have implemented a comparator class, which will sort the list
			  based on the CPU burst length of each process. The shortest will be at front. 
			  
SRT			- like SPN that uses linked list for queueing but the time interrupt handler in the schedule class will decrement the remaining time of 
			  each process by 100 ticks. Then, we will do a comparison between the remaining time of the running process and the first one in the list,
			  since the list is sorted by remaining time and the first one is always the one with the least amount of remaining time. 

HRRN		- when someone calls poll(), we update the response ratio of each process on the queue and sort the queue by that updated ratio before deciding which
			  process to remove.  
			  
-- The above 3 scheduling method requires the C program call predictCPU() before entering the loop, therefore when we sort the list, we check if every process has 
   called the predictCPU() so we have some idea on how to sort them, if there's one process on the list that hasn't initiated the syscall, we will yield currentThread
   and it run till they reach the syscall and sort the list after that.		
   
Feedback    - consists of 4 FIFO queue and the 5th is a RR queue. During every time interrupt, process's quantum is decrement by 100, when it reaches 0 it will be yield and
			  add to the next lower queue.
			  


To run our program configuration: -v flag file_name -scheduling method 
and in the testDriver class, change variable "count" to create the desired number of process
for every 100,000 ticks we display the instantaneous load average up to that point 

for example, running file name test using round round would be -v test -rr

Round Robin 	-rr
SPN 			-spn
SRT 			-srt
HRRN 			-hrrn
Feedback 		-feedback
FIFO			default, no flag needed

We think most of our graphs were close to the graph in Stalling's textbook but the feedback's graph was off from what's in the book.
For each graph, we generate them based on the result of 200 processes and saved them in .xlsx format and plot the graph based on collected data.